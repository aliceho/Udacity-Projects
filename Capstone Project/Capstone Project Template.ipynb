{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "# Project Title\n",
    "### Data Engineering Capstone Project\n",
    "\n",
    "#### Project Summary\n",
    "We will use Spark SQL to aggregate the data sources.\n",
    "\n",
    "The project follows the follow steps:\n",
    "* Step 1: Scope the Project and Gather Data\n",
    "* Step 2: Explore and Assess the Data\n",
    "* Step 3: Define the Data Model\n",
    "* Step 4: Run ETL to Model the Data\n",
    "* Step 5: Complete Project Write Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# importing packages and files\n",
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "from state_abb import state_udf, abbrev_state, abbrev_state_udf,city_code_udf,city_codes\n",
    "from immigration_codes import country_udf\n",
    "from pyspark.sql import SparkSession, SQLContext, GroupedData\n",
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Building Spark Session\n",
    "spark = SparkSession.builder.\\\n",
    "config(\"spark.jars.packages\",\"saurfang:spark-sas7bdat:2.0.0-s_2.11\")\\\n",
    ".enableHiveSupport().getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Building SQL context object\n",
    "sqlContext = SQLContext(spark)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 1: Scope the Project and Gather Data\n",
    "\n",
    "#### Scope \n",
    "The project will show the movement of immigration. We will pull data from various data sources to create fact and dimension tables, in order to show how immigration moves.\n",
    "\n",
    "#### Describe and Gather Data \n",
    "1. **U.S. City Demographic Data:** comes from OpenSoft and includes data by city, state, age, population, veteran status and race.\n",
    "2. **I94 Immigration Data:** comes from the US National Tourism and Trade Office and includes details on incoming immigrants and their ports of entry.\n",
    "3. **Airport Code Table:** comes from datahub.io and includes airport codes and corresponding cities.\n",
    "4. **World Temperature Data:** comes from kaggle and includes data on temperature changes in the U.S. since 1850"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Read in the data here\n",
    "demog=spark.read.format(\"csv\").option(\"header\", \"true\").option(\"delimiter\", \";\").load(\"us-cities-demographics.csv\")\n",
    "airport=spark.read.format(\"csv\").option(\"header\", \"true\").load(\"airport-codes_csv.csv\")\n",
    "temperatureData=spark.read.format(\"csv\").option(\"header\", \"true\").load(\"GlobalLandTemperaturesByState.csv\")\n",
    "df_spark=spark.read.format('com.github.saurfang.sas.spark').load(\"../../data/18-83510-I94-Data-2016/i94_jun16_sub.sas7bdat\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 2: Explore and Assess the Data\n",
    "#### Explore the Data \n",
    "Identify data quality issues.\n",
    "\n",
    "#### Cleaning Steps\n",
    "1. Filtering avg temperature data for the U.S. where year == 2012. Creating fields with year, month, fahrenheit. Running abbreviations function. Dropping duplicates.\n",
    "2. Removing 'nulls'. Converting i94res codes to 'country of origin'. Selecting significant columns from the immigration. Dropping duplicates.\n",
    "3. Sorting city demographic data. Calculating percentages and selecting percentages fields. Dropping duplicates.\n",
    "4. Filtering airport data for \"small_airport\". Using substring to return the state code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "# Filtering avg temperature data for the U.S. where year == 2012. Creating fields with year, month, fahrenheit. Running abbreviations function. Dropping duplicates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#temperature data by state\n",
    "usTemperatures=temperatureData.filter(temperatureData[\"country\"]==\"United States\")\\\n",
    ".filter(year(temperatureData[\"dt\"])==2012)\\\n",
    ".withColumn(\"year\",year(temperatureData[\"dt\"]))\\\n",
    ".withColumn(\"month\",month(temperatureData[\"dt\"]))\\\n",
    ".withColumn(\"avg_temp_fahrenheit\",temperatureData[\"AverageTemperature\"]*9/5+32)\\\n",
    ".withColumn(\"state_abbrev\",state_udf(temperatureData[\"State\"]))\n",
    "\n",
    "new_Temperatures=usTemperatures.select(\"year\",\"month\",round(col(\"AverageTemperature\"),1).alias(\"avg_temp_celcius\"),\\\n",
    "                                       round(col(\"avg_temp_fahrenheit\"),1).alias(\"avg_temp_fahrenheit\"),\n",
    "                                       \"state_abbrev\",\"State\",\"Country\").dropDuplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+----------------+-------------------+------------+-------------+-------------+\n",
      "|year|month|avg_temp_celcius|avg_temp_fahrenheit|state_abbrev|        State|      Country|\n",
      "+----+-----+----------------+-------------------+------------+-------------+-------------+\n",
      "|2012|    2|             0.2|               32.4|          MA|Massachusetts|United States|\n",
      "|2012|    9|            24.2|               75.6|          MS|  Mississippi|United States|\n",
      "|2012|    6|            21.7|               71.1|          VA|     Virginia|United States|\n",
      "+----+-----+----------------+-------------------+------------+-------------+-------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_Temperatures.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "# Removing 'nulls'. Converting i94res codes to 'country of origin'. Selecting significant columns from the immigration. Dropping duplicates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#immigration data by state with origin\n",
    "i94_data=df_spark.filter(df_spark.i94addr.isNotNull())\\\n",
    ".filter(df_spark.i94res.isNotNull())\\\n",
    ".filter(col(\"i94addr\").isin(list(abbrev_state.keys())))\\\n",
    ".filter(col(\"i94port\").isin(list(city_codes.keys())))\\\n",
    ".withColumn(\"origin_country\",country_udf(df_spark[\"i94res\"]))\\\n",
    ".withColumn(\"dest_state_name\",abbrev_state_udf(df_spark[\"i94addr\"]))\\\n",
    ".withColumn(\"i94yr\",col(\"i94yr\").cast(\"integer\"))\\\n",
    ".withColumn(\"i94mon\",col(\"i94mon\").cast(\"integer\"))\\\n",
    ".withColumn(\"city_port_name\",city_code_udf(df_spark[\"i94port\"]))\n",
    "\n",
    "new_I94_Data=i94_data.select(\"cicid\",col(\"i94yr\").alias(\"year\"),col(\"i94mon\").alias(\"month\"),\\\n",
    "                             \"origin_country\",\"i94port\",\"city_port_name\",col(\"i94addr\").alias(\"state_code\"),\"dest_state_name\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+-----+--------------+-------+------------------+----------+---------------+\n",
      "|cicid|year|month|origin_country|i94port|    city_port_name|state_code|dest_state_name|\n",
      "+-----+----+-----+--------------+-------+------------------+----------+---------------+\n",
      "| 41.0|2016|    6|   SOUTH KOREA|    SFR|SAN FRANCISCO     |        CA|     California|\n",
      "| 42.0|2016|    6|   SOUTH KOREA|    SFR|SAN FRANCISCO     |        CA|     California|\n",
      "| 45.0|2016|    6|       ROMANIA|    HOU|HOUSTON           |        TX|          Texas|\n",
      "+-----+----+-----+--------------+-------+------------------+----------+---------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_I94_Data.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "# Sorting city demographic data. Calculating percentages and selecting percentages fields. Dropping duplicates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#U.S. demographic data by state\n",
    "#Calculating percentages of each numeric column and creating new columns.\n",
    "demog_data=demog\\\n",
    ".withColumn(\"Median Age\",col(\"Median Age\").cast(\"float\"))\\\n",
    ".withColumn(\"pct_male_pop\",demog[\"Male Population\"]/demog[\"Total Population\"]*100)\\\n",
    ".withColumn(\"pct_female_pop\",demog[\"Female Population\"]/demog[\"Total Population\"]*100)\\\n",
    ".withColumn(\"pct_veterans\",demog[\"Number of Veterans\"]/demog[\"Total Population\"]*100)\\\n",
    ".withColumn(\"pct_foreign_born\",demog[\"Foreign-born\"]/demog[\"Total Population\"]*100)\\\n",
    ".withColumn(\"pct_race\",demog[\"Count\"]/demog[\"Total Population\"]*100)\\\n",
    ".orderBy(\"State\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Selecting columns with new calculated percentages and state names.\n",
    "new_demog_data=demog_data.select(\"State\",col(\"State Code\").alias(\"state_code\"),\\\n",
    "                                 col(\"Median Age\").alias(\"median_age\"),\\\n",
    "                                 \"pct_male_pop\",\\\n",
    "                                 \"pct_female_pop\",\\\n",
    "                                 \"pct_veterans\",\\\n",
    "                                 \"pct_foreign_born\",\\\n",
    "                                 \"Race\",\\\n",
    "                                 \"pct_race\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Select columns with new calculated percentages and state names.\n",
    "new_demog_data=demog_data.select(\"State\",col(\"State Code\").alias(\"state_code\"),\\\n",
    "                                 col(\"Median Age\").alias(\"median_age\"),\\\n",
    "                                 \"pct_male_pop\",\\\n",
    "                                 \"pct_female_pop\",\\\n",
    "                                 \"pct_veterans\",\\\n",
    "                                 \"pct_foreign_born\",\\\n",
    "                                 \"Race\",\\\n",
    "                                 \"pct_race\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#pivoting the Race column\n",
    "pivot_demog_data=new_demog_data.groupBy(\"State\",\"state_code\",\"median_age\",\"pct_male_pop\",\\\n",
    "                                    \"pct_female_pop\",\"pct_veterans\",\\\n",
    "                                    \"pct_foreign_born\").pivot(\"Race\").avg(\"pct_race\")\n",
    "\n",
    "#changing the header name of the race fields for spark compatibility.\n",
    "pivot_demog_data=pivot_demog_data.select(\"State\",\"state_code\",\"median_age\",\"pct_male_pop\",\"pct_female_pop\",\"pct_veterans\",\"pct_foreign_born\",\\\n",
    "                                         col(\"American Indian and Alaska Native\").alias(\"native_american\"),\\\n",
    "                                         col(\"Asian\"),col(\"Black or African-American\").alias(\"Black\"),\\\n",
    "                                         col(\"Hispanic or Latino\").alias(\"hispanic_or_latino\"),\"White\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Finding the avg of each column per state. The avg function will distort the column names but will fix next step.\n",
    "pivot=pivot_demog_data.groupBy(\"State\",\"state_code\").avg(\"median_age\",\"pct_male_pop\",\"pct_female_pop\",\\\n",
    "                                                       \"pct_veterans\",\"pct_foreign_born\",\"native_american\",\\\n",
    "                                                       \"Asian\",\"Black\",\"hispanic_or_latino\",\"White\").orderBy(\"State\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Rounding the percentages and fixing column names\n",
    "pivot=pivot.select(\"State\",\"state_code\",round(col(\"avg(median_age)\"),1).alias(\"median_age\"),\\\n",
    "                  round(col(\"avg(pct_male_pop)\"),1).alias(\"pct_male_pop\"),\\\n",
    "                   round(col(\"avg(pct_female_pop)\"),1).alias(\"pct_female_pop\"),\\\n",
    "                   round(col(\"avg(pct_veterans)\"),1).alias(\"pct_veterans\"),\\\n",
    "                   round(col(\"avg(pct_foreign_born)\"),1).alias(\"pct_foreign_born\"),\\\n",
    "                   round(col(\"avg(native_american)\"),1).alias(\"native_american\"),\\\n",
    "                   round(col(\"avg(Asian)\"),1).alias(\"Asian\"),\\\n",
    "                   round(col(\"avg(hispanic_or_latino)\"),1).alias(\"hispanic_or_latino\"),\\\n",
    "                   round(col(\"avg(Black)\"),1).alias(\"Black\"),\\\n",
    "                   round(col('avg(White)'),1).alias('White')\n",
    "                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+----------+------------+--------------+------------+----------------+---------------+-----+------------------+-----+-----+\n",
      "|  State|state_code|median_age|pct_male_pop|pct_female_pop|pct_veterans|pct_foreign_born|native_american|Asian|hispanic_or_latino|Black|White|\n",
      "+-------+----------+----------+------------+--------------+------------+----------------+---------------+-----+------------------+-----+-----+\n",
      "|Alabama|        AL|      36.2|        47.2|          52.8|         6.8|             5.1|            0.8|  2.9|               3.6| 45.0| 52.0|\n",
      "| Alaska|        AK|      32.2|        51.2|          48.8|         9.2|            11.1|           12.2| 12.3|               9.1|  7.7| 71.2|\n",
      "|Arizona|        AZ|      35.0|        48.8|          51.2|         6.6|            12.6|            2.8|  5.1|              28.8|  6.0| 82.7|\n",
      "+-------+----------+----------+------------+--------------+------------+----------------+---------------+-----+------------------+-----+-----+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pivot.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "# Filtering airport data for \"small_airport\". Using substring to return the state code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#U.S. airport data by state\n",
    "airport_data=airport.filter(airport[\"type\"]==\"small_airport\")\\\n",
    ".filter(airport[\"iso_country\"]==\"US\")\\\n",
    ".withColumn(\"iso_region\",substring(airport[\"iso_region\"],4,2))\\\n",
    ".withColumn(\"elevation_ft\",col(\"elevation_ft\").cast(\"float\"))\n",
    "\n",
    "#Find average elevation per state\n",
    "airport_data_elevation=airport_data.groupBy(\"iso_country\",\"iso_region\").avg(\"elevation_ft\")\n",
    "\n",
    "#Select relevant columns and drop duplicates\n",
    "new_airport_data=airport_data_elevation.select(col(\"iso_country\").alias(\"country\"),\\\n",
    "                                               col(\"iso_region\").alias(\"state\"),\\\n",
    "                                               round(col(\"avg(elevation_ft)\"),1).alias(\"avg_elevation_ft\")).orderBy(\"iso_region\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+----------------+\n",
      "|country|state|avg_elevation_ft|\n",
      "+-------+-----+----------------+\n",
      "|     US|   AK|           545.1|\n",
      "|     US|   AL|           414.6|\n",
      "|     US|   AR|           488.4|\n",
      "+-------+-----+----------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_airport_data.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 3: Define the Data Model\n",
    "#### 3.1 Conceptual Data Model\n",
    "I have used star schema to construct this data model. In a star schema database design, the dimensions are linked only through the central fact table. When two dimension tables are used in a query, only one join path, intersecting the fact table, exists between those two tables. This design feature enforces accurate and consistent query results.\n",
    "\n",
    "##### **Dimension Tables**\n",
    "###### Airport Data by State\n",
    " * country\n",
    " * state\n",
    " * avg_elevation_ft \n",
    " \n",
    "###### U.S. Demographic by State\n",
    " * State\n",
    " * state_code\n",
    " * median_age\n",
    " * pct_male_pop\n",
    " * pct_female_pop\n",
    " * pct_veterans\n",
    " * pct_foreign_born\n",
    " * native_american\n",
    " * Asian\n",
    " * hispanic_or_latino\n",
    " * Black\n",
    " * White\n",
    " \n",
    "###### Immigration Data by State with Origin\n",
    " * cicid\n",
    " * year\n",
    " * month\n",
    " * origin_country\n",
    " * i94port\n",
    " * city_port_name\n",
    " * state_code\n",
    " * dest_state_name\n",
    "\n",
    "###### Temperature Data by State\n",
    " * year\n",
    " * month\n",
    " * avg_temp_celcius\n",
    " * avg_temp_fahrenheit\n",
    " * state_abbrev\n",
    " * State\n",
    " * Country\n",
    "\n",
    "#### **Fact Table**\n",
    " * year\n",
    " * immig_month\n",
    " * immig_origin\n",
    " * to_immig_state\n",
    " * to_immig_state_count\n",
    " * avg_temp_fahrenheit\n",
    " * avg_elevation_ft\n",
    " * pct_foreign_born\n",
    " * native_american\n",
    " * Asian\n",
    " * hispanic_or_latino\n",
    " * Black\n",
    " * White\n",
    "\n",
    "#### 3.2 Mapping Out Data Pipelines\n",
    "1. Dimension tables are created from the cleansed data.\n",
    "2. Fact table are SQL query with joins to dimension tables.\n",
    "3. Fact table is converted back to a spark dataframe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 4: Run Pipelines to Model the Data \n",
    "#### 4.1 Create the data model\n",
    "Build the data pipelines to create the data model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Dimension Tables\n",
    "new_I94_Data.createOrReplaceTempView(\"immigration\")\n",
    "pivot.createOrReplaceTempView(\"demographics\")\n",
    "new_airport_data.createOrReplaceTempView(\"airport\")\n",
    "new_Temperatures.createOrReplaceTempView(\"temperature\")\n",
    "\n",
    "# Unlimited time for SQL joins. Parquet writes.\n",
    "sqlContext.setConf(\"spark.sql.autoBroadcastJoinThreshold\", \"0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Fact table\n",
    "immigration_to_states=spark.sql(\"\"\"SELECT \n",
    "                                    m.year,\n",
    "                                    m.month AS immig_month,\n",
    "                                    m.origin_country AS immig_origin,\n",
    "                                    m.dest_state_name AS to_immig_state,\n",
    "                                    COUNT(m.state_code) AS to_immig_state_count,\n",
    "                                    t.avg_temp_fahrenheit,\n",
    "                                    a.avg_elevation_ft,\n",
    "                                    d.pct_foreign_born,\n",
    "                                    d.native_american,\n",
    "                                    d.Asian,\n",
    "                                    d.hispanic_or_latino,\n",
    "                                    d.Black,\n",
    "                                    d.White\n",
    "                                    \n",
    "                                    FROM immigration m JOIN temperature t ON m.state_code=t.state_abbrev AND m.month=t.month\n",
    "                                    JOIN demographics d ON d.state_code=t.state_abbrev\n",
    "                                    JOIN airport a ON a.state=t.state_abbrev\n",
    "                                    \n",
    "                                    GROUP BY m.year,m.month, m.origin_country,\\\n",
    "                                    m.dest_state_name,m.state_code,t.avg_temp_fahrenheit,a.avg_elevation_ft,\\\n",
    "                                    d.pct_foreign_born,d.native_american,\\\n",
    "                                    d.Asian,d.hispanic_or_latino,\\\n",
    "                                    d.hispanic_or_latino,d.White,\\\n",
    "                                    d.Black\n",
    "                                    \n",
    "                                    ORDER BY m.origin_country,m.state_code\n",
    "                                    \n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----------+------------+--------------+--------------------+-------------------+----------------+----------------+---------------+-----+------------------+-----+-----+\n",
      "|year|immig_month|immig_origin|to_immig_state|to_immig_state_count|avg_temp_fahrenheit|avg_elevation_ft|pct_foreign_born|native_american|Asian|hispanic_or_latino|Black|White|\n",
      "+----+-----------+------------+--------------+--------------------+-------------------+----------------+----------------+---------------+-----+------------------+-----+-----+\n",
      "|2016|          6| AFGHANISTAN|      Arkansas|                   1|               78.6|           488.4|            10.7|            1.8|  4.1|              14.2| 21.8| 68.0|\n",
      "|2016|          6| AFGHANISTAN|       Arizona|                   1|               79.2|          3098.0|            12.6|            2.8|  5.1|              28.8|  6.0| 82.7|\n",
      "|2016|          6| AFGHANISTAN|    California|                  31|               69.4|          1261.4|            27.6|            1.7| 17.9|              37.8|  7.5| 62.7|\n",
      "+----+-----------+------------+--------------+--------------------+-------------------+----------------+----------------+---------------+-----+------------------+-----+-----+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "immigration_to_states.toDF('year', 'immig_month', 'immig_origin', 'to_immig_state', \\\n",
    "          'to_immig_state_count', 'avg_temp_fahrenheit', 'avg_elevation_ft',\\\n",
    "          'pct_foreign_born', 'native_american', 'Asian', 'hispanic_or_latino', 'Black', 'White').show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.2 Data Quality Checks\n",
    "Explain the data quality checks you'll perform to ensure the pipeline ran as expected. These could include:\n",
    " * Integrity constraints on the relational database (e.g., unique key, data type, etc.)\n",
    " * Unit tests for the scripts to ensure they are doing the right thing\n",
    " * Source/Count checks to ensure completeness\n",
    " \n",
    "Run Quality Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+-------+-----+\n",
      "| year|month|country|state|\n",
      "+-----+-----+-------+-----+\n",
      "|false|false|  false|false|\n",
      "+-----+-----+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Perform quality checks here\n",
    "#Checking for null value\n",
    "immigration_to_states.select(isnull('year').alias('year'),\\\n",
    "                             isnull('immig_month').alias('month'),\\\n",
    "                             isnull('immig_origin').alias('country'),\\\n",
    "                             isnull('to_immig_state').alias('state')).dropDuplicates().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "| 3214208|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Count the total number of immigrants from the source table.\n",
    "spark.sql('SELECT COUNT(*) FROM immigration').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+\n",
      "|fact_table_count|\n",
      "+----------------+\n",
      "|         3207230|\n",
      "+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Count the total number of people immigrated to the United States from the Fact Table.\n",
    "immigration_to_states.select(sum('to_immig_state_count').alias('fact_table_count')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.3 Data dictionary \n",
    "* Read file titled \"tables_definition\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Step 5: Complete Project Write Up\n",
    "1. Since the data used in this project is small, I have used Apache spark for to read, transform, create data outputs for further analysis. \n",
    "2. The data should be updated monthly. This gives the latest data for government and organizations.\n",
    "3. Write a description of how you would approach the problem differently under the following scenarios:\n",
    " * The data was increased by 100x: We can use Hadoop for faster prcessing.\n",
    " * The data populates a dashboard that must be updated on a daily basis by 7am every day: We can use Airflow to create a schedule that can    run a distributed update on all the tables.\n",
    " * The database needed to be accessed by 100+ people: We can use AWS for increased capacity"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
